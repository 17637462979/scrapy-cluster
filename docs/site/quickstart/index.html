<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Quick Start - Scrapy Cluster</title>
  

  <link rel="shortcut icon" href="../img/favicon.ico">

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">

  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script>
  <script src="../js/theme.js"></script> 

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Scrapy Cluster</a>
        
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
      <ul class="current">
    
        
            <li class="toctree-l1 ">
                <a class="" href="..">Home</a>
                
            </li>
        

    
        
            <li class="toctree-l1 current">
                <a class="current" href="./">Quick Start</a>
                
                    <ul>
                    
                        <li class="toctree-l3"><a href="#quick-start">Quick start</a></li>
                        
                    
                    </ul>
                
            </li>
        

    
        
            <li class="toctree-l1 ">
                <a class="" href="../crawler/">Crawler</a>
                
            </li>
        

    
        
            <li class="toctree-l1 ">
                <a class="" href="../kafka-monitor/">Kafka Monitor</a>
                
            </li>
        

    
        
            <li class="toctree-l1 ">
                <a class="" href="../redis-monitor/">Redis Monitor</a>
                
            </li>
        

    
</ul>

      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Scrapy Cluster</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
    <li>Quick Start</li>
      <li class="wy-breadcrumbs-aside">
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              <h2 id="quick-start">Quick start</h2>
<p><em>This guide does not go into detail as to how everything works, but hopefully will get you scraping quickly. For more information about each process works please see the README's within each sub project.</em></p>
<p>1) Make Sure you have Apache Zookeeper, Apache Kafka, and Redis up and running on your cluster. For more information about standing those up, please refer to the official project documentation.</p>
<p>2) Download and unzip the project from the sidebar or <a href="https://github.com/istresearch/scrapy-cluster/archive/master.zip">here</a>. Lets assume our project is now in <code>~/scrapy-cluster</code></p>
<p>3) You will now need to configure the following four settings files:</p>
<p>3a) <code>~/scrapy-cluster/kafka-monitor/settings-crawling.py</code></p>
<p>Add your specific configuraiton to <code>REDIS_HOST</code>, <code>REDIS_PORT</code>, and <code>KAFKA_HOSTS</code>.</p>
<p>For example:</p>
<pre><code>REDIS_HOST = 'server-1'
REDIS_PORT = 6379
KAFKA_HOSTS = 'server-2:9092'
</code></pre>

<p>This is used to determine where the Kafka Monitor will listen to incoming crawl requests and where send them.</p>
<p>3b) <code>~/scrapy-cluster/kafka-monitor/settings-actions.py</code></p>
<p>Add your specific configuraiton to <code>REDIS_HOST</code>, <code>REDIS_PORT</code>, and <code>KAFKA_HOSTS</code>.</p>
<p>For example:</p>
<pre><code>REDIS_HOST = 'server-1'
REDIS_PORT = 6379
KAFKA_HOSTS = 'server-2:9092'
</code></pre>

<p>Notice this is very similar to the first settings file, but the other parameters are different. This settings file is used to listen for action requests and determines where to send them.</p>
<p>3c) <code>~/scrapy-cluster/crawler/crawling/settings.py</code></p>
<p>Add your specific configuraiton to <code>REDIS_HOST</code>, <code>REDIS_PORT</code>, and <code>KAFKA_HOSTS</code>.</p>
<p>For example:</p>
<pre><code>REDIS_HOST = 'server-1'
REDIS_PORT = 6379
KAFKA_HOSTS = 'server-2:9092'
</code></pre>

<p>This settings file is used to configure Scrapy. It uses all of the configurations already built in with the project, with a few more to get us into cluster mode. The new settings are utilized by the scheduler and item pipeline.</p>
<p>3d) <code>~/scrapy-cluster/redis-monitor/settings.py</code></p>
<p>Add your specific configuraiton to <code>REDIS_HOST</code>, <code>REDIS_PORT</code>, and <code>KAFKA_HOSTS</code>.</p>
<p>For example:</p>
<pre><code>REDIS_HOST = 'server-1'
REDIS_PORT = 6379
KAFKA_HOSTS = 'server-2:9092'
</code></pre>

<p>This last settings file is used to get information out of the redis queue, and tells the redis monitor where to point.</p>
<p>4) At this point we can start up either a bare bones cluster, or a fully operational cluster:</p>
<p><em>Note you can append <code>&amp;</code> to the end of the following commands to run them in the background, but we recommend you open different terminal windows to first get a feel of how the cluster operates.</em></p>
<p><strong>Bare Bones:</strong>
  * The Kafka Monitor for Crawling:
<code>python kafka-monitor.py run -s settings_crawling.py</code></p>
<ul>
<li>
<p>A crawler:
<code>scrapy runspider crawling/spiders/link_spider.py</code></p>
</li>
<li>
<p>The dump utility located in Kafka-Monitor to see your results
<code>python kafkadump.py dump demo.crawled_firehose --host=server-2:9092</code></p>
</li>
</ul>
<p><strong>Fully Operational:</strong>
  * The Kafka Monitor for Crawling:
<code>python kafka-monitor.py run -s settings_crawling.py</code></p>
<ul>
<li>
<p>The Kafka Monitor for Actions:
<code>python kafka-monitor.py run -s settings_actions.py</code></p>
</li>
<li>
<p>The Redis Monitor:
<code>python redis-monitor.py</code></p>
</li>
<li>
<p>A crawler (1+):
<code>scrapy runspider crawling/spiders/link_spider.py</code></p>
</li>
<li>
<p>The dump utility located in Kafka Monitor to see your crawl results
<code>python kafkadump.py dump demo.crawled_firehose --host=server-2:9092</code></p>
</li>
<li>
<p>The dump utility located in Kafka Monitor to see your action results
<code>python kafkadump.py dump demo.outbound_firehose --host=server-2:9092</code></p>
</li>
</ul>
<p>5) We now need to feed the cluster a crawl request. This is done via the same kafka-monitor python script, but with different command line arguements.</p>
<p><code>python kafka-monitor.py feed '{"url": "http://istresearch.com", "appid":"testapp", "crawlid":"abc123"}' -s settings_crawling.py</code></p>
<p>You will see the following output on the command line for that successful request:</p>
<pre><code>=&gt; feeding JSON request into demo.incoming_urls...
{
    &quot;url&quot;: &quot;http://istresearch.com&quot;,
    &quot;crawlid&quot;: &quot;abc123&quot;,
    &quot;appid&quot;: &quot;testapp&quot;
}
=&gt; done feeding request.
</code></pre>

<ul>
<li>If this command hangs, it means the script cannot connect to Kafka</li>
</ul>
<p>6) After a successful request, the following chain of events should occur in order:
  1. The Kafka monitor will receive the crawl request and put it into Redis
  2. The spider periodically checks for new requests, and will pull the request from the queue and process it like a normal Scrapy spider.
  3. After the scraped item is yielded to the Scrapy item pipeline, the Kafka Pipeline object will push the result back to Kafka
  4. The Kafka Dump utility will read from the resulting output topic, and print out the raw scrape object it received</p>
<p>7) The Redis Monitor utility is useful for learning about your crawl while it is being processed and sitting in redis, so we will pick a larger site so we can see how it works (Note this requires a full deployment).</p>
<p>Crawl Request:</p>
<p><code>python kafka-monitor.py feed '{"url": "http://dmoz.org", "appid":"testapp", "crawlid":"abc1234", "maxdepth":1}' -s settings_crawling.py</code></p>
<p>Now send an <code>info</code> action request to see what is going on with the crawl:</p>
<p><code>python kafka-monitor.py feed -s settings_actions.py '{"action":"info", "appid":"testapp", "uuid":"someuuid", "crawlid":"abc1234", "spiderid":"link"}'</code></p>
<p>The following things will occur for this action request:
  1. The Kafka monitor will receive the action request and put it into Redis
  2. The Redis Monitor will act on the info request, and tally the current pending requests for the particular <code>spiderid</code>, <code>appid</code>, and <code>crawlid</code>
  3. The Redis Monitor will send the result back to Kafka
  4. The Kafka Dump utility monitoring the actions will receive a result similar to the following:</p>
<pre><code>{u'server_time': 1430170027, u'crawlid': u'abc1234', u'total_pending': 48, u'low_priority': -19, u'high_priority': -9, u'appid': u'testapp', u'uuid': u'someuuid'}
</code></pre>

<p>In this case we had 48 urls pending in the queue, so yours may be slightly different.</p>
<p>8) If the crawl from step 7 is still running, lets stop it by issuing a <code>stop</code> action request (Note this requires a full deployment).</p>
<p>Action Request:</p>
<p><code>python kafka-monitor.py feed -s settings_actions.py '{"action":"stop", "appid":"testapp", "uuid":"someuuid", "crawlid":"abc1234", "spiderid":"link"}'</code></p>
<p>The following things will occur for this action request:
  1. The Kafka monitor will receive the action request and put it into Redis
  2. The Redis Monitor will act on the stop request, and purge the current pending requests for the particular <code>spiderid</code>, <code>appid</code>, and <code>crawlid</code>
  3. The Redis Monitor will blacklist the <code>crawlid</code>, so no more pending requests can be generated from the spiders or application
  3. The Redis Monitor will send the purge total result back to Kafka
  4. The Kafka Dump utility monitoring the actions will receive a result similar to the following:</p>
<pre><code>{u'action': u'stop', u'total_purged': 48, u'spiderid': u'link', u'crawlid': u'abc1234', u'appid': u'testapp'}
</code></pre>

<p>In this case we had 48 urls removed from the queue. Those pending requests are now completely removed from the system and the spider will go back to being idle.</p>
<hr />
<p>Hopefully you now have a working Scrapy Cluster that allows you to submit jobs to the queue, receive information about your crawl, and stop a crawl if it gets out of control. For a more in depth look at each of the components, please refer to the README's within each subproject.
  * <a href="https://github.com/istresearch/scrapy-cluster/blob/master/crawler/README.md">Crawler README</a>
  * <a href="https://github.com/istresearch/scrapy-cluster/blob/master/kafka-monitor/README.md">Kafka Monitor README</a>
  * <a href="https://github.com/istresearch/scrapy-cluster/blob/master/redis-monitor/README.md">Redis Monitor README</a></p>
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../crawler/" class="btn btn-neutral float-right" title="Crawler"/>Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href=".." class="btn btn-neutral" title="Home"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
    <!-- Copyright etc -->
    </p>
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
        
      <span><a href=".." style="color: #fcfcfc;">&laquo; Previous</a></span>
      <span style="margin-left: 15px"><a href="../crawler/" style="color: #fcfcfc">Next &raquo;</a></span>
    </span>
</div>

<!--
MkDocs version  : 0.12.2
Docs Build Date : 2015-05-06 20:52:11.684178
-->
</body>
</html>